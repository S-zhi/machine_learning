## 机器学习——梯度下降（二）

在前面的内容中我们知道了单维度的斜率下降，但是在实际生活中模型往往不会那么简单的，当多维度的变量出现的时候我们应当如何解决这又是一个难题。于是我们又要开始思考如何扩展到二维上？

下面我们打算引入一个比最基本的感知器难一点的一个模型，斜距式 。

<center>  y  =  k  *  x  +  b  </center> 

首先我们拿梯度下降的思路想，这里有两个未知数，那么我们就假设一个三维空间，在这个三维空间上，是怎么样的内容呢， 那我们再来看一下，分别从两个未知数的变化上来看， 在 x 为自变量上，y的值显然是一个二次函数，b 为自变量的时候呢， y 的值还是一个二次函数 。 那他们合起来是什么样子的？

<img src="D:\machine learning\4,梯度下降（二）\assets\image-20231205170725244.png" alt="image-20231205170725244" />

就像这样一样是一个曲面，在这个曲面上存在一个绝对最低点，我们如何达到这个 E 的绝对最低点，就是整个程序的解题的关键！

***

#### 一， 多维度梯度下降

对于多维度的梯度下降还是和原来一样，我们需要保证几个条件的一致性，从而得到一种递推关系，首先在探讨多维梯度下降的时候我们就应当知道，一些必要的数学知识，我们原来一维只用在对当前的最优点方向移动就可以，当我们遇到了多维度，我们向着这两个的分向量的和向量移动是否是正确的呢？答案是肯定的

<img src="D:\machine learning\4,梯度下降（二）\assets\image-20231205171630248.png" alt="image-20231205171630248" style="zoom:50%;" />

经过两个向量的和之后，所生成的方向也就是我们所要求的方向，使用这个进行逐步式的梯度下降所得到得结果也就必然是正确的了。

下面讲解对于截距式的数学推导，首先是我们要推导出来他的误差公式。

<center>E = (a * x + b - y) ** 2     整理得 ： E =   a ** 2 * x ** 2 + 2 * (a * x)(b - y ) + (b - y )** 2 </center>

  看起来还是很抽象，我们只用注意 当 b ，a两个偏方向的向量，下面我们来具体的看一下公式



<img src="D:\machine learning\4,梯度下降（二）\assets\image-20231205174924244.png" alt="image-20231205174924244" style="zoom:50%;" /> 

使用这个公式对 a 和 b 分别进行更新，就可以实现二维上的梯度下降。

> 1. 梯度下降
>
>    介绍 ： 梯度下降就是所谓的使用一个多维度的斜率，分别对当前的维度进行更新，完成对总维度的更新，总的看起来就是向最低点进行趋向，看起来就i想按照一点点梯度进行下降一样，故叫做梯度下降。
>
>    <img src="D:\machine learning\4,梯度下降（二）\assets\image-20231205175732524.png" alt="image-20231205175732524" style="zoom:33%;" />
>
> 2. 反向传播 
>
>    介绍 ： 我们在每次更新完成之后一些值，再用这些值来进行预测，使用这种作用方式就是反向传播



